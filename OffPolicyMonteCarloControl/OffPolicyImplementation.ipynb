{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Off-policy MC Control for estimating the optimal policy (Sutton and Barto, section 5.7, page 111)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using OpenAI Gym's CliffWalking environment.\n",
    "\n",
    "Action Space - move up (0), move right (1), move down (2), move left (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CliffWalking-v0\")\n",
    "        self.Q = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.C = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        # self.policy = [np.argmax(observation) for observation in self.Q]\n",
    "        self.policy = np.ones(self.env.observation_space.n)\n",
    "    \n",
    "    def generate_episode(self, policy):\n",
    "        episode = []\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "        action = policy()\n",
    "\n",
    "        while True:\n",
    "            new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                return episode\n",
    "            \n",
    "            episode.append([state, action, reward])\n",
    "\n",
    "            state = new_state\n",
    "            action = policy()\n",
    "    \n",
    "    def randomPolicy(self):\n",
    "        return np.random.randint(self.env.action_space.n)\n",
    "    \n",
    "    def goRightEpsilonGreedily(self):\n",
    "        if np.random.rand() > 0.5:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "        \n",
    "        return 1\n",
    "\n",
    "    def control(self, num_episodes):\n",
    "        gamma = 0.95\n",
    "        for _ in tqdm(range(num_episodes)):\n",
    "            episode = self.generate_episode(self.goRightEpsilonGreedily)\n",
    "            G = 0\n",
    "            W = 1\n",
    "            for step_idx in range(len(episode) - 1, -1, -1):\n",
    "                step = episode[step_idx]\n",
    "                G = gamma * G + step[2]\n",
    "                self.C[step[0]][step[1]] = self.C[step[0]][step[1]] + W\n",
    "                self.Q[step[0]][step[1]] = self.Q[step[0]][step[1]] + W / self.C[step[0]][step[1]] * (G - self.Q[step[0]][step[1]])\n",
    "                self.policy[step[0]] = np.argmax(self.Q[step[0]])\n",
    "                if step[1] != self.policy[step[0]]:\n",
    "                    break\n",
    "                W = W / 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 572.09it/s]\n"
     ]
    }
   ],
   "source": [
    "a = Agent()\n",
    "a.control(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0., -1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, -1, False, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "env.reset()\n",
    "env.step(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b71e2ea7fd88c01752403482f8b390a2bb97379ffce9aede2d7f28ae0381b030"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
