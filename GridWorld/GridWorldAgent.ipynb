{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of policy iteration (Sutton and Barto, section 4.3, page 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    '''\n",
    "    Initializes the agent\n",
    "    @param rows - The number of rows in the grid world\n",
    "    @param columns - The number of columns in the grid world\n",
    "    @param terminal_state - The final state the agent is trying to find the best path to\n",
    "    '''\n",
    "    def __init__(self, rows, columns, terminal_state, theta=0.05, gamma=0.85):\n",
    "        # self.grid = np.random.rand(rows, columns) * -1 # Initializes\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "        self.values = np.zeros((rows, columns)) # Initializes all state values to zero\n",
    "        self.policies = self.initializePolicies()\n",
    "        # self.policies = self.initializePolicies() # Initializes policies for each cell,\n",
    "        #                                           # policies[row][column] is an array where the value\n",
    "        #                                           # at index 0 is the probability that up is the optimal choice,\n",
    "        #                                           # 1 is right, 2 is down, 3 is left (clockwise around Cartesian)\n",
    "        self.terminal_state = terminal_state # Where the end state is located on the grid\n",
    "        self.theta = theta # A value close to zero signifying completion, determines the accuracy of the policy estimation\n",
    "        self.gamma = gamma # A value signifying by how much to discount future rewards \n",
    "\n",
    "    '''\n",
    "    Initializes the policies for each cell in the grid world\n",
    "    '''\n",
    "    def initializePolicies(self):\n",
    "        policies = np.empty((self.rows, self.columns, 4))\n",
    "\n",
    "        # Fill the array with random values that sum to 1\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.columns):\n",
    "                policies[i, j] = np.random.dirichlet(np.ones(4))\n",
    "                # policies[i, j] = np.zeros(4)\n",
    "                # policies[i, j][np.random.randint(4)] = 1\n",
    "\n",
    "        return policies\n",
    "\n",
    "    def isValidState(self, coords):\n",
    "        return coords[0] >= 0 and coords[0] < self.rows and coords[1] >= 0 and coords[1] < self.columns\n",
    "\n",
    "    '''\n",
    "    Gets the four available successor states\n",
    "    Note: some of the returned successor states may be invalid, a validity check is needed\n",
    "    @param coords - coordinates in tuple form (x, y) of our state\n",
    "    @return a list of successors\n",
    "    '''\n",
    "    def getAvailableSuccessorStates(self, coords):\n",
    "        successors = []\n",
    "\n",
    "        successors.append((coords[0] - 1, coords[1]))\n",
    "        successors.append((coords[0], coords[1] + 1))\n",
    "        successors.append((coords[0] + 1, coords[1]))\n",
    "        successors.append((coords[0], coords[1] - 1))\n",
    "\n",
    "        return successors\n",
    "\n",
    "    '''\n",
    "    Get the discounted value of each successor state\n",
    "    @return a numpy array of successor state values\n",
    "    '''\n",
    "    def getDiscountedValuesForSuccessors(self, state):\n",
    "        values = np.zeros(4)\n",
    "        \n",
    "        successors = self.getAvailableSuccessorStates(state) # Get possible next states\n",
    "        \n",
    "        value = 0\n",
    "        for successor in range(len(successors)):\n",
    "            if self.isValidState(successors[successor]):\n",
    "                probability_successor_chosen = self.policies[state][successor]\n",
    "                successor_value = self.getReward(successors[successor]) + self.gamma * self.values[successors[successor]]\n",
    "                values[successor] = probability_successor_chosen * successor_value\n",
    "\n",
    "        return values\n",
    "\n",
    "    '''\n",
    "    Update the policy based on the values of successor states\n",
    "    '''\n",
    "    def updatePolicy(self, coords):\n",
    "        successors = self.getAvailableSuccessorStates(coords)\n",
    "        values = np.ones(4) / 4\n",
    "        for successor in range(len(successors)):\n",
    "            if self.isValidState(successors[successor]):\n",
    "                values[successor] = self.values[successors[successor][0]][successors[successor][1]] + self.getReward(successors[successor])\n",
    "            else:\n",
    "                values[successor] = 0\n",
    "        \n",
    "        sum = np.sum(values)\n",
    "        policies = np.ones(4) / 4 if sum == 0 else values / sum\n",
    "\n",
    "        self.policies[coords[0]][coords[1]] = policies\n",
    "\n",
    "    '''\n",
    "    Gets the reward of a particular state\n",
    "    High reward if terminal state, no reward otherwise\n",
    "    '''\n",
    "    def getReward(self, state):\n",
    "        if state[0] == self.terminal_state[0] and state[1] == self.terminal_state[1]: # Terminal state\n",
    "            return 100\n",
    "        return 0\n",
    "        \n",
    "\n",
    "    '''\n",
    "    Performs a policy evaluation step\n",
    "    Update our value function based on our current policy\n",
    "    '''\n",
    "    def policyEvaluation(self):\n",
    "        while True:\n",
    "            delta = 0 # This is the check to know when to stop evaluation\n",
    "            for coords, old_value in np.ndenumerate(self.values): # Iterate over states - in the form ((row, column), value)\n",
    "                \n",
    "                value = np.sum(self.getDiscountedValuesForSuccessors(coords)) # Cell value is equal to the sum of the discounted successor state values\n",
    "                self.values[coords] = value\n",
    "\n",
    "                delta = max(delta, np.absolute(old_value - value)) # Get the change in the value\n",
    "\n",
    "            if delta < self.theta: # Check if the update was significant\n",
    "                break\n",
    "    \n",
    "    '''\n",
    "    Performs a policy improvement step\n",
    "    Generate a new, improved policy based on our new value function\n",
    "    '''\n",
    "    def policyImprovement(self):\n",
    "        while True:\n",
    "            stable = True\n",
    "            for coords, value in np.ndenumerate(self.values): # Iterate over all states\n",
    "                old_action = np.argmax(self.policies[coords]) # Get the previous policy value\n",
    "                self.updatePolicy(coords) # Update the policy based on the updated value function\n",
    "                action = np.argmax(self.policies[coords]) # Get the best action according to our new policy\n",
    "                if old_action != action: # If our policy has changed, it is unstable, so we perform a policy evaluation step\n",
    "                    stable = False\n",
    "                    \n",
    "            if not stable:\n",
    "                self.policyEvaluation()\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    '''\n",
    "    Outputs the policy in a readable format\n",
    "    '''\n",
    "    def outputPolicy(self):\n",
    "\n",
    "        mapping = {0: \"Up\", 1: \"Right\", 2: \"Down\", 3: \"Left\"}\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for row in range(len(self.policies)):\n",
    "            row_output = []\n",
    "            for cell in range(len(self.policies[row])):\n",
    "                if row == self.terminal_state[0] and cell == self.terminal_state[1]:\n",
    "                    print('{:^6s}'.format(\"End\"), end=\"\")\n",
    "                else:\n",
    "                    print('{:^6s}'.format(mapping[np.argmax(self.policies[row][cell])]), end=\"\")\n",
    "            print()\n",
    "\n",
    "    def valueUpdate(self, state):\n",
    "        values = np.zeros(4)\n",
    "        \n",
    "        successors = self.getAvailableSuccessorStates(state) # Get possible next states\n",
    "        \n",
    "        value = 0\n",
    "        for successor in range(len(successors)):\n",
    "            if self.isValidState(successors[successor]):\n",
    "                probability_successor_chosen = self.policies[state[0]][state[1]][successor]\n",
    "                successor_value = self.getReward(successors[successor]) + self.gamma * self.values[successors[successor]]\n",
    "                values[successor] = probability_successor_chosen * successor_value\n",
    "\n",
    "        return values\n",
    "\n",
    "    '''\n",
    "    Value iteration alternative to policy iteration\n",
    "    '''\n",
    "    def valueIteration(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for coords, old_value in np.ndenumerate(self.values): # Iterate over each cell\n",
    "                value = np.amax(self.getDiscountedValuesForSuccessors(coords)) # Take the maximum discounted value from the successor states\n",
    "                self.values[coords] = value # Update current state value\n",
    "                delta = max(delta, np.absolute(old_value - value))\n",
    "\n",
    "            if delta < self.theta: # If our policy is stable\n",
    "                break   \n",
    "        \n",
    "        for coords, old_value in np.ndenumerate(self.values): # Update the policy at each set of coordinates\n",
    "            self.updatePolicy(coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent(10, 10, (2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.valueIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right Right  Down Right Right  Down  Left  Left  Left  Left \n",
      "Right  Down  Down  Down  Down  Down  Left  Left  Left  Left \n",
      "Right Right Right Right  End   Left  Left  Left  Left  Left \n",
      "Right   Up    Up    Up    Up    Up   Left  Left  Left  Left \n",
      "Right Right   Up    Up    Up    Up    Up    Up    Up   Left \n",
      "  Up    Up    Up  Right   Up   Left   Up    Up    Up    Up  \n",
      "  Up  Right   Up  Right   Up   Left  Left   Up   Left  Left \n",
      "Right Right Right Right   Up   Left  Left   Up   Left  Left \n",
      "Right Right Right Right   Up    Up   Left  Left  Left  Left \n",
      "Right Right   Up    Up    Up    Up    Up    Up    Up   Left \n"
     ]
    }
   ],
   "source": [
    "a.outputPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.policyImprovement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0.1,  0.4,  0.3,  4.8, 22.4,  1.3,  0.1,  0. ,  0. ],\n",
       "       [ 0.1,  0.2,  1.4,  3.6, 19.2, 46.5, 11.7,  0.5,  0. ,  0. ],\n",
       "       [ 0. ,  1.9,  5.8, 30.9, 34.1, 78. ,  4.4,  1. ,  0.2,  0. ],\n",
       "       [ 0.1,  0.4,  1. ,  4.6, 10.2,  7.8,  6.2,  0.9,  0.2,  0. ],\n",
       "       [ 0. ,  0.2,  0.5,  0.5,  0.8,  0.5,  0.3,  0.1,  0.1,  0. ],\n",
       "       [ 0. ,  0. ,  0.1,  0. ,  0.6,  0.1,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0.4,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0.2,  0.1,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.values.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right Right  Down Right Right  Down  Left  Left  Left  Left \n",
      "Right  Down  Down  Down  Down  Down  Left  Left  Left  Left \n",
      "Right Right Right Right  End   Left  Left  Left  Left  Left \n",
      "Right   Up    Up    Up    Up    Up   Left  Left  Left  Left \n",
      "Right Right   Up    Up    Up    Up    Up    Up    Up   Left \n",
      "  Up    Up    Up  Right   Up   Left   Up    Up    Up    Up  \n",
      "  Up  Right   Up  Right   Up   Left  Left   Up   Left  Left \n",
      "Right Right Right Right   Up   Left  Left   Up   Left  Left \n",
      "Right Right Right Right   Up    Up   Left  Left  Left  Left \n",
      "Right Right   Up    Up    Up    Up    Up    Up    Up   Left \n"
     ]
    }
   ],
   "source": [
    "a.outputPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b71e2ea7fd88c01752403482f8b390a2bb97379ffce9aede2d7f28ae0381b030"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
