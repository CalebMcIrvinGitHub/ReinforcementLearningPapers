{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Monte Carlo ES (Sutton and Barto, section 5.3, page 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OpenAI's gym, blackjack has an observation space consisting of a 3-tuple of\n",
    "\n",
    "The player's current sum\n",
    "\n",
    "The value of dealer's one showing card (1 - 10)\n",
    "\n",
    "Whether or not the player has an ace (0 or 1) - important because aces can be either 1 or 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, gamma=0.85):\n",
    "        self.env = gym.make(\"Blackjack-v1\", natural=False) # Blackjack environment from the OpenAI gym\n",
    "        self.sum_size = self.env.observation_space[0].n # Number of possibilities for the player's current sum\n",
    "        self.dealer_size = self.env.observation_space[1].n # Number of possibilities for the dealer's face up card\n",
    "        self.gamma = gamma\n",
    "        self.policy = self.initializeBlackjackPolicy()\n",
    "        self.Q = self.initializeQ()\n",
    "        self.returns = self.initializeReturns()\n",
    "\n",
    "    '''\n",
    "    Create an empty returns list\n",
    "    '''\n",
    "    def initializeReturns(self):\n",
    "        dealer_possibilities = []\n",
    "        player_sum_possibilities = []\n",
    "        actions = []\n",
    "\n",
    "        for i in range(self.env.action_space.n):\n",
    "            actions.append([])\n",
    "\n",
    "        for i in range(self.dealer_size):\n",
    "            dealer_possibilities.append(actions)\n",
    "        \n",
    "        for j in range(self.sum_size):\n",
    "            player_sum_possibilities.append(dealer_possibilities)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return player_sum_possibilities\n",
    "    \n",
    "    '''\n",
    "    Initializes each state's policy to a random initial value\n",
    "    '''\n",
    "    def initializeBlackjackPolicy(self):\n",
    "        policy = np.empty((self.sum_size, self.dealer_size), dtype=int)\n",
    "        for sum in range(self.sum_size):\n",
    "            for dealer_value in range(self.dealer_size):\n",
    "                policy[sum, dealer_value] = 0\n",
    "\n",
    "        return policy\n",
    "    \n",
    "    '''\n",
    "    Creates a Q function of zeros for each state action pair\n",
    "    '''\n",
    "    def initializeQ(self):\n",
    "        Q = np.empty((self.sum_size, self.dealer_size, self.env.action_space.n))\n",
    "        for sum in range(self.sum_size):\n",
    "            for dealer_value in range(self.dealer_size):\n",
    "                Q[sum, dealer_value] = np.zeros(self.env.action_space.n)\n",
    "\n",
    "        return Q\n",
    "    \n",
    "    '''\n",
    "    Returns a random action to take and gain experience from\n",
    "    Primarily for testing purposes, inefficient in practice\n",
    "    '''\n",
    "    def getRandomAction(self, observation):\n",
    "        return np.random.randint(0, self.env.action_space.n)\n",
    "\n",
    "    def getActionFromPolicy(self, observation):\n",
    "        S_t_dim_0 = observation[0] - 1 # subtract 1 for indexing\n",
    "        S_t_dim_1 = observation[1] - 1 # subtract 1 for indexing\n",
    "        return self.policy[S_t_dim_0][S_t_dim_1]\n",
    "    \n",
    "    '''\n",
    "    Generates an action based on the policy to follow\n",
    "    '''\n",
    "    def generateEpisode(self, actionType=getActionFromPolicy):\n",
    "        observation = self.env.reset()\n",
    "        observation = (observation[0][0], observation[0][1])\n",
    "        episode = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = actionType(self, observation)\n",
    "            new_observation, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "            new_state = (observation[0], observation[1])\n",
    "            episode.append((new_state, action, reward))\n",
    "            observation = new_observation\n",
    "\n",
    "            done = terminated or truncated\n",
    "        return episode\n",
    "\n",
    "    def updateAfterEpisode(self, episode):\n",
    "        G = 0\n",
    "        for step in range(len(episode)):\n",
    "            S_t_dim_0 = episode[step][0][0]\n",
    "            S_t_dim_1 = episode[step][0][1]\n",
    "            \n",
    "            A_t = episode[step][1]\n",
    "\n",
    "            reward = episode[step][2]\n",
    "            G = self.gamma * G + reward\n",
    "\n",
    "            if episode[step][0] in episode[:step]: # Implement first look only\n",
    "                continue\n",
    "\n",
    "            self.returns[S_t_dim_0][S_t_dim_1][A_t].append(G)\n",
    "            self.Q[S_t_dim_0, S_t_dim_1, A_t] = np.average(self.returns[S_t_dim_0][S_t_dim_1][A_t])\n",
    "            self.policy[S_t_dim_0][S_t_dim_1] = np.argmax(self.Q[S_t_dim_0, S_t_dim_1])\n",
    "\n",
    "    def monteCarloES(self, gamma=0.85, num_episodes=100000):\n",
    "        for ep in tqdm(range(num_episodes)):\n",
    "            observation, info = self.env.reset()\n",
    "            episode = self.generateEpisode()\n",
    "            self.updateAfterEpisode(episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Agent()\n",
    "a.monteCarloES()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.returns[30][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1827600080466707"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(a.returns[30][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.Q[30, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcd4837add793386cd67e25a441830de5b4a5ceabdd5dcba5a317ceae751db5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
